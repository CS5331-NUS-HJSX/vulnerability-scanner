import urllib
from urllib import parse as urlparse
import socket
import os
import cgi
import requests
import datetime

from bs4 import BeautifulSoup

def parseLink(baseUrl, linkTag):
    for (key, value) in linkTag.attrs.items():
        if key == 'href':
            # checking if it is an external link. If so, do not include it
            if 'http' in value:
                data = urlparse.urlparse(baseUrl)
                domain_seq = (data.scheme, '://', data.netloc)
                domain = "".join(domain_seq)
                if domain in value:  # the link domain is same as the site domain, then include it
                    return value

            else:
                newUrl = urlparse.urljoin(baseUrl, value)
                return newUrl

    return ""

tokenKeys = ["csrftoken"]

class CrawlerHttpSubmission(object):
    def __init__(self, pageUrl, url, method = "GET", data=None):
        parsed = urlparse.urlparse(url)

        self.baseUrl = parsed.scheme + "://" + parsed.netloc
        self.path = parsed.path
        self.pageUrl = pageUrl
        self.url = url
        self.method = method
        self.data = data
    
    def submit(self):
        crawler = CrawlerHttp()
        if(self.method.lower() == "get"):
            return crawler.send(self.url, self.method, {}, self.data)
        else:
            return crawler.send(self.url, self.method, {}, None, self.data)

    def json(self):
        jsonData = {}
        jsonData["location"] = self.baseUrl
        jsonData["path"] = self.path
        jsonData["page"] = self.pageUrl
        jsonData["url"] = self.url
        jsonData["method"] = self.method
        jsonData["data"] = self.data
        return jsonData

    def alterEmptyData(self, value):
        for key in self.data:
            if len(self.data[key]) == 0:
                self.data[key] = value

    def alterData(self, value):
        for key in self.data:
            if not key in tokenKeys:
                self.data[key] = value

    def encodeData(self):
        self.data = urlparse.urlencode(self.data)

    def duplicate(self):
        newdata = {}
        for key in self.data:
            newdata[key] = self.data[key]
        return CrawlerHttpSubmission(self.pageUrl, self.url, self.method, newdata)

    def __str__(self):
        return "[" + self.method + "] " + self.url + " data count: " + str(len(self.data))

class CrawlerHttpInput(object):
    name = ""
    type = ""
    value = ""

    def __init__(self, dom):
        self.dom = dom
        self.name = ""
        self.type = ""
        self.value = ""

        if dom.has_attr("name"):
            self.name = dom.attrs["name"] 

        if dom.has_attr("value"):
            self.value = dom.attrs["value"]

        if dom. has_attr("type"):
            self.type = dom.attrs["type"]

    def isSubmittable(self):
        if(self.type.lower() == "submit"):
            return False
        return True

    def __str__(self):
        return "[" + self.type + "] " + self.name + " : " + self.value 

class CrawlerHttpForm(object):
    input = []
    url = ""
    action = ""
    method = "GET"

    def __init__(self, dom, baseurl):
        self.dom = dom

        if dom.has_attr("action"):
            self.action = dom.attrs["action"]

        self.url = urlparse.urljoin(baseurl, self.action)

        if dom.has_attr("method"):
            self.method = dom.attrs["method"]

        inputInForm = dom.find_all("input")
        for input in inputInForm:
            self.input = self.input + [CrawlerHttpInput(input)]
    
    def getInputDictionary(self):
        result = {}
        for inputValues in self.input:
            if (not inputValues.name in result) and inputValues.isSubmittable():
                result[inputValues.name] = inputValues.value
        return result

    def __str__(self):
        return "[" + self.method + "] " + self.url + " input count: " + str(len(self.input))

class CrawlerHttpResponse(object):
    resp = None

    def __init__(self, url, requests_resp, peer, timestamp):
        self.resp = requests_resp
        self.peer = peer
        self.timestamp = timestamp
        self.url = url

    def getPage(self):
        """Return the content of the page in unicode."""
        if self.resp.encoding:
            return self.resp.text
        else:
            return self.resp.content

    def getRawPage(self):
        """Return the content of the page in raw bytes."""
        return self.resp.content

    def getCode(self):
        """Return the HTTP Response code ."""
        return str(self.resp.status_code)

    def getHeaders(self):
        """Return the HTTP headers of the Response."""
        return self.resp.headers

    def getPageCode(self):
        """Return a tuple of the content and the HTTP Response code."""
        return self.getPage(), self.getCode()

    def getEncoding(self):
        """Return the detected encoding for the page."""
        if self.resp.encoding:
            return self.resp.encoding.upper()
        return None

    def getApparentEncoding(self):
        """Return the detected encoding for the page."""
        if self.resp.apparent_encoding:
            return self.resp.apparent_encoding.upper()
        return None

    def setEncoding(self, new_encoding):
        """Change the encoding (for getPage())"""
        self.resp.encoding = new_encoding

    def getPeer(self):
        """Return the network address of the server that delivered this Response.
        This will always be a socket_object.getpeername() return value, which is
        normally a (ip_address, port) tuple."""
        return self.peer

    def getTimestamp(self):
        """Return a datetime.datetime object describing when this response was
        received."""
        return self.timestamp

    def getLink(self):
        beautifulSoupDocument = BeautifulSoup(self.getPage(), "lxml")
        links = []
        linkTags = []
        if(not beautifulSoupDocument.body is None):
           linkTags = beautifulSoupDocument.body.find_all('a')
        for linkTag in linkTags:            
            currentLink = parseLink(self.url, linkTag)
            if currentLink and (not currentLink.isspace()):
                links = links + [currentLink]
        
        return links

    def getLinkCount(self):
        return len(self.getLink())

    def getForm(self):
        beautifulSoupDocument = BeautifulSoup(self.getPage(), "lxml")
        formTags = []
        if(not beautifulSoupDocument.body is None):
            formTags = beautifulSoupDocument.body.find_all('form')
        parsedForm = []
        for form in formTags:
            parsedForm = parsedForm + [CrawlerHttpForm(form, self.url)]
        return parsedForm

    def getFormCount(self):
        return len(self.getForm())

    def getParameter(self):
        parsed = urlparse.urlparse(self.url)
        parameters = urlparse.parse_qs(parsed.query)
        parameterDictionary = {}
        for k in parameters:
            if(not k in parameterDictionary):
                parameterDictionary[k] = parameters[k][0]
        return parameterDictionary

    def getParameterCount(self):
        return len(self.getParameter())

    def getBaseUrl(self):
        parsed = urlparse.urlparse(self.url)
        baseUrl = urlparse.urljoin(parsed.scheme + "://" + parsed.netloc, parsed.path)
        return baseUrl

    def getSubmission(self):
        submissions = []
        forms = self.getForm()
        if(len(forms) > 0):
            for form in forms:
                submissions = submissions + [CrawlerHttpSubmission(self.url, form.url, form.method, form.getInputDictionary())]
        parameters = self.getParameter()
        if(len(parameters) > 0):
            submissions = submissions + [CrawlerHttpSubmission(self.url, self.getBaseUrl(), "GET", parameters)]
        return submissions

    def __str__(self):
        return self.url

class CrawlerHttp(object):
    def __init__(self):
        self.h = requests.Session()
        for adapter_protocol in self.h.adapters:
            self.h.adapters[adapter_protocol].max_retries = 1
        
        self.proxies = {}
        self.auth_credentials = []
        self.auth_method = "basic"
        self.timeout = 6.0
        self.cookiejar = {}
        self.verify_ssl = False
        self.sslErrorOccured = False
        self.base_headers = {"user-agent": "Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)"}

        self.configured = 0

    def send(self, target, method="GET", headers={}, get_params=None, post_params=None, file_params=None) -> CrawlerHttpResponse:
        """Send a HTTP Request. GET or POST (if post_params is set)."""
        resp = None
        _headers = self.base_headers.copy()
        _headers.update(headers)

        get_data = None
        if isinstance(get_params, str):
            get_data = get_params
        elif isinstance(get_params, list):
            get_data = self.encode(get_params)
        elif isinstance(get_params, dict):
            get_data = self.encode(get_params.items())

        if isinstance(post_params, str):
            post_data = post_params
        elif isinstance(post_params, list):
            post_data = self.encode(post_params)
        elif isinstance(post_params, dict):
            post_data = self.encode(post_params.items())

        file_data = None
        if isinstance(file_params, tuple) or isinstance(file_params, list):
            file_data = file_params
        
        try:
            if method.lower() == "get":
                    resp = self.h.get(target, params=get_data, headers=_headers, timeout=self.timeout, allow_redirects=False, verify=self.verify_ssl)
            else:
                if method.lower() == "post":
                    if not file_data:
                        _headers.update({'content-type': 'application/x-www-form-urlencoded'})

                    resp = self.h.post(target, params=get_data, data=post_data, files=file_data, headers=_headers, timeout=self.timeout, allow_redirects=False, verify=self.verify_ssl)
                else:
                    resp = self.h.request(method, target, params=get_data, data=post_data, files=file_data, headers=_headers, timeout=self.timeout, allow_redirects=False, verify=self.verify_ssl)

        except Exception as e:
            print(e)

        if resp is None:
            return None
            
        return CrawlerHttpResponse(target, resp, "", datetime.datetime.now())

    @staticmethod
    def quote(url):
        """Encode a string with hex representation (%XX) for special characters."""
        return urlparse.quote(url)

    @staticmethod
    def encode(params_list):
        """Encode a sequence of two-element lists or dictionary into a URL query string."""
        encoded_params = []
        for k, v in params_list:
            # not safe: '&=#' with of course quotes...
            k = urlparse.quote(k, safe='/%[]:;$()+,!?*')
            v = urlparse.quote(v, safe='/%[]:;$()+,!?*')
            encoded_params.append("%s=%s" % (k, v))
        return "&".join(encoded_params)

    def uqe(self, params_list):  # , encoding = None):
        """urlencode a string then interpret the hex characters (%41 will give 'A')."""
        return urlparse.unquote(self.encode(params_list))  # , encoding))

    @staticmethod
    def escape(url):
        """Change special characters in their html entities representation."""
        return cgi.escape(url, quote=True).replace("'", "%27")

    def setTimeOut(self, timeout=6.0):
        """Set the time to wait for a response from the server."""
        self.timeout = timeout
        socket.setdefaulttimeout(self.timeout)

    def getTimeOut(self):
        """Return the timeout used for HTTP requests."""
        return self.timeout

    def setProxy(self, proxy=""):
        """Set a proxy to use for HTTP requests."""
        url_parts = urlparse.urlparse(proxy)
        protocol = url_parts.scheme
        host = url_parts.netloc
        if protocol in ["http", "https"]:
            if host:
                self.proxies[protocol] = "%s://%s/" % (protocol, host)
        self.h.proxies = self.proxies

    def setAuthCredentials(self, auth_credentials):
        """Set credentials to use if the website require an authentication."""
        self.auth_credentials = auth_credentials
        # Force reload
        self.setAuthMethod(self.auth_method)

    def setAuthMethod(self, auth_method):
        """Set the authentication method to use for the requests."""
        self.auth_method = auth_method
        if len(self.auth_credentials) == 2:
            username, password = self.auth_credentials
            if self.auth_method == "basic":
                from requests.auth import HTTPBasicAuth
                self.h.auth = HTTPBasicAuth(username, password)
            elif self.auth_method == "digest":
                from requests.auth import HTTPDigestAuth
                self.h.auth = HTTPDigestAuth(username, password)

    def addCustomHeader(self, key, value):
        """Set a HTTP header to use for every requests"""
        self.base_headers[key] = value
