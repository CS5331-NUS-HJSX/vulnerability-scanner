from html.parser import HTMLParser  
from urllib.request import urlopen  
from urllib import parse

from crawlerhttp import CrawlerHttp, CrawlerHttpResponse

from bs4 import BeautifulSoup

crawler = CrawlerHttp()

def spider(url, iteration):  
    pagesToVisit = [url]
    pagesVisited = []
    numberVisited = 0
    crawledPages = []

    while pagesToVisit != []:
        url = pagesToVisit[0]
        pagesToVisit = pagesToVisit[1:]

        if url not in pagesVisited:
            numberVisited = numberVisited +1
            try:
                crawledDocument = crawler.send(url)
                links = crawledDocument.getLink()

                if numberVisited < iteration:
                    pagesToVisit = sorted(list(set(pagesToVisit + links)))

                pagesVisited = pagesVisited + [url]
                crawledPages = crawledPages + [crawledDocument]
            except Exception as e:
                print(" **Failed!**")
                print(e)

    return crawledPages

pages = spider("http://192.168.56.101/", 100)

def formatData(data, length):
    dataString = str(data)
    dataLengthFormat = "{:<" + str(length) + "}"
    return dataLengthFormat.format(dataString[:length])

print(formatData("Page Visited" , 100) + formatData("Form Count", 15) + formatData("Link Count", 15) + formatData("Input Count", 15)  + formatData("Param Count", 15))
for page in pages:
    print(formatData(page,100) + formatData(page.getFormCount(), 15) + formatData(page.getLinkCount(), 15) + formatData(page.getInputCount(), 15) + formatData(page.getParameterCount(), 15))

