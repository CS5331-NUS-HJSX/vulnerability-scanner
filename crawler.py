from html.parser import HTMLParser  
from urllib.request import urlopen  
from urllib import parse

from crawlerhttp import CrawlerHttp, CrawlerHttpResponse

from bs4 import BeautifulSoup

import json

crawler = CrawlerHttp()

def spider(url, iteration):  
    pagesToVisit = [url]
    pagesVisited = []
    numberVisited = 0
    crawledPages = []

    while pagesToVisit != []:
        url = pagesToVisit[0]
        pagesToVisit = pagesToVisit[1:]

        if url not in pagesVisited:
            numberVisited = numberVisited +1
            try:
                crawledDocument = crawler.send(url)
                links = crawledDocument.getLink()

                if numberVisited < iteration:
                    pagesToVisit = sorted(list(set(pagesToVisit + links)))

                pagesVisited = pagesVisited + [url]
                crawledPages = crawledPages + [crawledDocument]
            except Exception as e:
                print(" **Failed!**")
                print(e)

    return crawledPages

pages = spider("http://192.168.1.75/", 100)

def formatData(data, length):
    dataString = str(data)
    dataLengthFormat = "{:<" + str(length) + "}"
    return dataLengthFormat.format(dataString[:length])

def getInputName(input_result_set):
    input_array = []
    for x in input_result_set:
        if x.has_attr("name"):
            input_obj = { "name": x.attrs["name"] }
            if x.has_attr("value"):
                input_obj["value"] = x.attrs["value"]
            input_array.append(input_obj)
    return input_array

def getFormData(form_result_set):
    form_array = []
    for x in form_result_set:
        form_obj = {}
        if x.has_attr("action"):
            form_obj["action"] = x["action"]
        if x.has_attr("method"):
            form_obj["method"] = x["method"]
        input_result_set = x.find_all("input")
        form_obj["input"] = getInputName(input_result_set)

        form_array.append(form_obj)

    return form_array

result_array = []
print(formatData("Page Visited" , 100) + formatData("Form Count", 15) + formatData("Link Count", 15) + formatData("Input Count", 15)  + formatData("Param Count", 15))
for page in pages:
    # print(formatData(page,100) + formatData(page.getFormCount(), 15) + formatData(page.getLinkCount(), 15) + formatData(page.getInputCount(), 15) + formatData(page.getParameterCount(), 15))
    data = {
        "url": page.url,
        "form": getFormData(page.getForm()),
        "link": page.getLinkCount(),
        "parameter": page.getParameter()
    }
    result_array.append(data);
    

    
with open('crawler_output.json', 'w') as outfile:  
    json.dump(result_array, outfile, indent=4)
