from html.parser import HTMLParser  
from urllib.request import urlopen  
from urllib import parse

from bs4 import BeautifulSoup

def parseDocument(url):
    response = urlopen(url)
    charset = response.headers.get_content_charset()
    if not (charset and (not charset.isspace())):
        charset = "utf-8"

    parsedHtml = BeautifulSoup("<html></html>", "lxml")

    if "text/html" in response.getheader('Content-Type'):
        htmlBytes = response.read()
        htmlString = htmlBytes.decode(charset)
        parsedHtml = BeautifulSoup(htmlString, "lxml")
    
    return parsedHtml

def getLink(baseUrl, linkTag):
    for (key, value) in linkTag.attrs.items():
        if key == 'href':
            newUrl = parse.urljoin(baseUrl, value)
            return newUrl
    return ""

def spider(url, iteration):  
    pagesToVisit = [url]
    pagesVisited = []
    numberVisited = 0
    crawledPages = []

    while pagesToVisit != []:
        url = pagesToVisit[0]
        pagesToVisit = pagesToVisit[1:]

        if url not in pagesVisited:
            numberVisited = numberVisited +1
            try:
                print(numberVisited, "Visiting:", url)
                beautifulSoupDocument = parseDocument(url)

                links = []
                linkTags = beautifulSoupDocument.body.find_all('a')
                for linkTag in linkTags:
                    currentLink = getLink(url, linkTag)
                    if currentLink and (not currentLink.isspace()):
                        links = links + [getLink(url, linkTag)]
                
                if numberVisited < iteration:
                    pagesToVisit = sorted(list(set(pagesToVisit + links)))

                pagesVisited = pagesVisited + [url]
                crawledPages = crawledPages + [{"url" : url, "dom" : beautifulSoupDocument}]
            except Exception as e:
                print(e)
                print(" **Failed!**")

    return crawledPages

pages = spider("https://www.facebook.com/", 10)
for page in pages:
    print(page["url"])
