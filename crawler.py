from html.parser import HTMLParser  
from urllib.request import urlopen  
from urllib import parse

from crawlerhttp import CrawlerHttp, CrawlerHttpResponse, CrawlerHttpInput, CrawlerHttpSubmission
from attacker import attack

from bs4 import BeautifulSoup

import json

crawler = CrawlerHttp()

def spider(url, iteration):  
    pagesToVisit = [url]
    pagesVisited = []
    numberVisited = 0
    crawledPages = []

    while pagesToVisit != []:
        url = pagesToVisit[0]
        pagesToVisit = pagesToVisit[1:]

        if url not in pagesVisited:
            numberVisited = numberVisited +1
            try:
                crawledDocument = crawler.send(url)
                links = crawledDocument.getLink()

                if numberVisited < iteration:
                    pagesToVisit = sorted(list(set(pagesToVisit + links)))

                pagesVisited = pagesVisited + [url]
                crawledPages = crawledPages + [crawledDocument]
            except Exception as e:
                print(" **Failed!**")
                print(e)
    return crawledPages

pages = spider("http://192.168.222.143/", 100)
#pages = spider("https://google.com/", 3)

def formatData(data, length):
    dataString = str(data)
    dataLengthFormat = "{:<" + str(length) + "}"
    return dataLengthFormat.format(dataString[:length])

def getInputName(input_result_set):
    input_array = []
    for x in input_result_set:
        if x.has_attr("name"):
            input_obj = { "name": x.attrs["name"] }
            if x.has_attr("value"):
                input_obj["value"] = x.attrs["value"]
            input_array.append(input_obj)
    return input_array

def getFormData(form_result_set):
    form_array = []
    for x in form_result_set:
        form_obj = {}
        if x.has_attr("action"):
            form_obj["action"] = x["action"]
        if x.has_attr("method"):
            form_obj["method"] = x["method"]
        input_result_set = x.find_all("input")
        form_obj["input"] = getInputName(input_result_set)

        form_array.append(form_obj)

    return form_array

result_array = []
allSubmissions = []

for page in pages:
    forms = page.getSubmission()
    allSubmissions = allSubmissions + forms
    formJson = []
    for submission in forms:
        formJson = formJson + [submission.json()]
        
    if(len(formJson) > 0):
        data = {
            "url": page.url,
            "form": formJson
        }
        result_array.append(data)
    
with open('crawler_output.json', 'w') as outfile:  
    json.dump(result_array, outfile, indent=4)

servercodeinjectionresult = attack(allSubmissions, "servercodeinjection.txt", "servercodeinjectionexpected.txt")

resultjson = []
for submission in servercodeinjectionresult:
    resultjson = resultjson + [submission.json()]
    
with open('result_server_code_injection.json', 'w') as outfile:  
    json.dump(resultjson, outfile, indent=4)
    